Bias and Variance
 Page 3 Machine Learning Yearning-Draft Andrew Ng
 20 Bias and Variance: The two big sources of error
Suppose your training, dev and test sets all come from the same distribution. Then you should always try to get more training data, since that can only improve performance, right?
Even though having more data can’t hurt, unfortunately it doesn’t always help as much as you might hope. It could be a waste of time to work on getting more data. So, how do you decide when to add data, and when not to bother?
There are two major sources of error in machine learning: bias and variance. Understanding them will help you decide whether adding data, as well as other tactics to improve performance, are a good use of time.
Suppose you hope to build a cat recognizer that has 5% error. Right now, your training set has an error rate of 15%, and your dev set has an error rate of 16%. In this case, adding training data probably won’t help much. You should focus on other changes. Indeed, adding more examples to your training set only makes it harder for your algorithm to do well on the training set. (We explain why in a later chapter.)
If your error rate on the training set is 15% (or 85% accuracy), but your target is 5% error (95% accuracy), then the first problem to solve is to improve your algorithm​’​s performance on your training set. Your dev/test set performance is usually worse than your training set performance. So if you are getting 85% accuracy on the examples your algorithm has seen, there’s no way you’re getting 95% accuracy on examples your algorithm hasn’t even seen.
Suppose as above that your algorithm has 16% error (84% accuracy) on the dev set. We break the 16% error into two components:
• First, the algorithm’s error rate on the training set. In this example, it is 15%. We think of this informally as the algorithm’s ​bias​.
• Second, how much worse the algorithm does on the dev (or test) set than the training set. In this example, it does 1% worse on the dev set than the training set. We think of this informally as the algorithm’s ​variance​.1
1 The field of statistics has more formal definitions of Bias and Variance that we won’t worry about. Roughly, the Bias is the error rate of your algorithm on your training set when you have a very large training set. The Variance is how much worse you do on the test set compared to the training set in
Page 4 Machine Learning Yearning-Draft Andrew Ng

 Some changes to a learning algorithm can address the first component of error—​bias​—and improve its performance on the training set. Some changes address the second component—​variance​—and help it generalize better from the training set to the dev/test sets.2 To select the most promising changes, it is incredibly useful to understand which of these two components of error is more pressing to address.
Developing good intuition about Bias and Variance will help you choose effective changes for your algorithm.
 this setting. When your error metric is mean squared error, you can write down formulas specifying these two quantities, and prove that Total Error = Bias + Variance. But for our purposes of deciding how to make progress on an ML problem, the more informal definition of Bias and Variance given here will suffice.
2 There are also some methods that can simultaneously reduce Bias and Variance, by making major changes to the system architecture. But these tend to be harder to identify and implement.
Page 5 Machine Learning Yearning-Draft Andrew Ng

 21 Examples of Bias and Variance
Consider our cat classification task. An “ideal” classifier (such as a human) might achieve nearly perfect performance in this task.
Suppose your algorithm performs as follows:
• Training error = 1%
• Dev error = 11%
What problem does it have? Applying the definitions from the previous chapter, we estimate the bias as 1%, and the variance as 10% (=11%-1%). Thus, it has ​high variance​. The classifier has very low training error, but it is failing to generalize to the dev set. This is also called ​overfitting​.
Now consider this:
• Training error = 15%
• Dev error = 16%
We estimate the bias as 15%, and variance as 1%. This classifier is fitting the training set poorly with 15% error, but its error on the dev set is barely higher than the training error. This classifier therefore has ​high bias​, but low variance. We say that this algorithm is underfitting​.
Now, consider this:
• Training error = 15%
• Dev error = 30%
We estimate the bias as 15%, and variance as 15%. This classifier has ​high bias and high variance​: It is doing poorly on the training set, and therefore has high bias, and its performance on the dev set is even worse, so it also has high variance. The overfitting/underfitting terminology is hard to apply here since the classifier is simultaneously overfitting and underfitting.
Page 6 Machine Learning Yearning-Draft Andrew Ng

 Finally, consider this:
• Training error = 0.5%
• Dev error = 1%
This classifier is doing well, as it has low bias and low variance. Congratulations on achieving this great performance!
Page 7 Machine Learning Yearning-Draft Andrew Ng

 22 Comparing to the optimal error rate
In our cat recognition example, the “ideal” error rate—that is, one achievable by an “optimal” classifier—is nearly 0%. A human looking at a picture would be able to recognize if it contains a cat almost all the time; thus, we can hope for a machine that would do just as well.
Other problems are harder. For example, suppose that you are building a speech recognition system, and find that 14% of the audio clips have so much background noise or are so unintelligible that even a human cannot recognize what was said. In this case, even the most “optimal” speech recognition system might have error around 14%.
Suppose that on this speech recognition problem, your algorithm achieves:
• Training error = 15%
• Dev error = 30%
The training set performance is already close to the optimal error rate of 14%. Thus, there is not much room for improvement in terms of bias or in terms of training set performance. However, this algorithm is not generalizing well to the dev set; thus there is ample room for improvement in the errors due to variance.
This example is similar to the third example from the previous chapter, which also had a training error of 15% and dev error of 30%. If the optimal error rate is ~0%, then a training error of 15% leaves much room for improvement. This suggests bias-reducing changes might be fruitful. But if the optimal error rate is 14%, then the same training set performance tells us that there’s little room for improvement in the classifier’s bias.
For problems where the optimal error rate is far from zero, here​’​s a more detailed breakdown of an algorithm​’​s error. Continuing with our speech recognition example above, the total dev set error of 30% can be broken down as follows (a similar analysis can be applied to the test set error):
• Optimal error rate (“unavoidable bias”)​: 14%. Suppose we decide that, even with the best possible speech system in the world, we would still suffer 14% error. We can think of this as the “unavoidable” part of a learning algorithm​’​s bias.
Page 8 Machine Learning Yearning-Draft Andrew Ng

 • Avoidable bias​: 1%. This is calculated as the difference between the training error and the optimal error rate.3
• Variance​: 15%. The difference between the dev error and the training error.
To relate this to our earlier definitions, Bias and Avoidable Bias are related as follows:4
Bias = Optimal error rate (“unavoidable bias”) + Avoidable bias
The “avoidable bias” reflects how much worse your algorithm performs on the training set than the “optimal classifier.”
The concept of variance remains the same as before. In theory, we can always reduce variance to nearly zero by training on a massive training set. Thus, all variance is “avoidable” with a sufficiently large dataset, so there is no such thing as “unavoidable variance.”
Consider one more example, where the optimal error rate is 14%, and we have:
• Training error = 15%
• Dev error = 16%
Whereas in the previous chapter we called this a high bias classifier, now we would say that error from avoidable bias is 1%, and the error from variance is about 1%. Thus, the algorithm is already doing well, with little room for improvement. It is only 2% worse than the optimal error rate.
We see from these examples that knowing the optimal error rate is helpful for guiding our next steps. In statistics, the optimal error rate is also called ​Bayes error rate​, or Bayes rate.
How do we know what the optimal error rate is? For tasks that humans are reasonably good at, such as recognizing pictures or transcribing audio clips, you can ask a human to provide labels then measure the accuracy of the human labels relative to your training set. This would give an estimate of the optimal error rate. If you are working on a problem that even
3 If this number is negative, you are doing better on the training set than the optimal error rate. This means you are overfitting on the training set, and the algorithm has over-memorized the training set. You should focus on variance reduction methods rather than on further bias reduction methods.
4 These definitions are chosen to convey insight on how to improve your learning algorithm. These definitions are different than how statisticians define Bias and Variance. Technically, what I define here as “Bias” should be called “Error we attribute to bias”; and “Avoidable bias” should be “error we attribute to the learning algorithm’s bias that is over the optimal error rate.”
 Page 9 Machine Learning Yearning-Draft Andrew Ng

humans have a hard time solving (e.g., predicting what movie to recommend, or what ad to show to a user) it can be hard to estimate the optimal error rate.
In the section “Comparing to Human-Level Performance (Chapters 33 to 35), I will discuss in more detail the process of comparing a learning algorithm’s performance to human-level performance.
In the last few chapters, you learned how to estimate avoidable/unavoidable bias and variance by looking at training and dev set error rates. The next chapter will discuss how you can use insights from such an analysis to prioritize techniques that reduce bias vs. techniques that reduce variance. There are very different techniques that you should apply depending on whether your project’s current problem is high (avoidable) bias or high variance. Read on!
Page 10 Machine Learning Yearning-Draft Andrew Ng
